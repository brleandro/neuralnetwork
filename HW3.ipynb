{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBLOml-YA1WW",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9de8ed2d1e76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m102\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m103\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-9de8ed2d1e76>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(self, n_epochs)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0my_onehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_onehot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-9de8ed2d1e76>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(self, n_epochs)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0my_onehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_onehot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_pydevd_bundle/pydevd_cython_darwin_37_64.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython_darwin_37_64.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_pydevd_bundle/pydevd_cython_darwin_37_64.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython_darwin_37_64.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_pydevd_bundle/pydevd_cython_darwin_37_64.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython_darwin_37_64.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_pydevd_bundle/pydevd_cython_darwin_37_64.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython_darwin_37_64.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Applications/PyCharm.app/Contents/helpers-pro/jupyter_debug/pydev_jupyter_plugin.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(plugin, pydb, frame, event, args, stop_info, arg, step_cmd)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuspend_jupyter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_debugger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_cmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mmain_debugger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/PyCharm.app/Contents/helpers/pydev/pydevd.py\u001b[0m in \u001b[0;36mdo_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads_suspended_single_notification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify_thread_suspended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuspend_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_this_thread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuspend_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_this_thread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/PyCharm.app/Contents/helpers/pydev/pydevd.py\u001b[0m in \u001b[0;36m_do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_internal_commands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel_async_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_current_thread_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class NN(object):\n",
    "    def __init__(self,\n",
    "                 hidden_dims=(512, 256),\n",
    "                 datapath='cifar10.pkl',\n",
    "                 n_classes=10,\n",
    "                 epsilon=1e-6,\n",
    "                 lr=7e-4,\n",
    "                 batch_size=1000,\n",
    "                 seed=None,\n",
    "                 activation=\"relu\",\n",
    "                 init_method=\"glorot\"''\n",
    "                 ):\n",
    "\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.n_hidden = len(hidden_dims)\n",
    "        self.datapath = datapath\n",
    "        self.n_classes = n_classes\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.init_method = init_method\n",
    "        self.seed = seed\n",
    "        self.activation_str = activation\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.train_logs = {'train_accuracy': [], 'validation_accuracy': [], 'train_loss': [], 'validation_loss': []}\n",
    "\n",
    "        if datapath is not None:\n",
    "            u = pickle._Unpickler(open(datapath, 'rb'))\n",
    "            u.encoding = 'latin1'\n",
    "            self.train, self.valid, self.test = u.load()\n",
    "        else:\n",
    "            self.train, self.valid, self.test = None, None, None\n",
    "\n",
    "    def initialize_weights(self, dims):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        print('seed', self.seed)\n",
    "        self.weights = {}\n",
    "        # self.weights is a dictionary with keys W1, b1, W2, b2, ..., Wm, Bm where m - 1 is the number of hidden layers\n",
    "        all_dims = [dims[0]] + list(self.hidden_dims) + [dims[1]]\n",
    "        for layer_n in range(1, self.n_hidden + 2):\n",
    "            # WRITE CODE HERE\n",
    "            self.weights[f'b{layer_n}'] = np.zeros((1, all_dims[layer_n]))\n",
    "            self.weights[f'W{layer_n}'] = np.random.uniform(low=-1/math.sqrt(all_dims[layer_n - 1]), high=1/math.sqrt(all_dims[layer_n - 1]), size=(all_dims[layer_n-1],all_dims[layer_n]))\n",
    "\n",
    "    def relu(self, x, grad=False):\n",
    "        if grad:\n",
    "            return x > 0\n",
    "        return np.maximum(np.zeros(x.shape[0]), x)\n",
    "\n",
    "    def sigmoid(self, x, grad=False):\n",
    "        if grad:\n",
    "            return np.exp(-x)/((1 + np.exp(-x))**2)\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def tanh(self, x, grad=False):\n",
    "        if grad:\n",
    "            return 1 - (((np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x)))**2)\n",
    "        return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "    def activation(self, x, grad=False):\n",
    "        if self.activation_str == \"relu\":\n",
    "            return self.relu(x, grad)\n",
    "        elif self.activation_str == \"sigmoid\":\n",
    "            return self.sigmoid(x, grad)\n",
    "        elif self.activation_str == \"tanh\":\n",
    "            return self.tanh(x, grad)\n",
    "        else:\n",
    "            raise Exception(\"invalid\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "    def softmax(self, x):\n",
    "        # Remember that softmax(x-C) = softmax(x) when C is a constant.\n",
    "        # WRITE CODE HERE\n",
    "        return np.exp(x - np.maximum(x))/sum(np.exp(x - np.maximum(x)))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        cache = {\"Z0\": x}\n",
    "        # cache is a dictionary with keys Z0, A0, ..., Zm, Am where m - 1 is the number of hidden layers\n",
    "        # Ai corresponds to the preactivation at layer i, Zi corresponds to the activation at layer i\n",
    "        # WRITE CODE HERE\n",
    "        x = np.array(x)\n",
    "        for i in range(1, self.n_hidden):\n",
    "            x = np.dot(self.weights[f'W{i}'].T, x) + self.weights[f'b{i}']\n",
    "            cache[f\"A{i}\"] = self.activation(x)\n",
    "            cache[f\"Z{i + 1}\"] = x\n",
    "        return cache\n",
    "\n",
    "    def backward(self, cache, labels):\n",
    "        output = cache[f\"Z{self.n_hidden + 1}\"]\n",
    "        grads = {}\n",
    "        # grads is a dictionary with keys dAm, dWm, dbm, dZ(m-1), dA(m-1), ..., dW1, db1\n",
    "        # WRITE CODE HERE\n",
    "        pass\n",
    "        return grads\n",
    "\n",
    "    def update(self, grads):\n",
    "        for layer in range(1, self.n_hidden + 2):\n",
    "            # WRITE CODE HERE\n",
    "            pass\n",
    "\n",
    "    def one_hot(self, y):\n",
    "        # WRITE CODE HERE\n",
    "        pass\n",
    "        return 0\n",
    "\n",
    "    def loss(self, prediction, labels):\n",
    "        prediction[np.where(prediction < self.epsilon)] = self.epsilon\n",
    "        prediction[np.where(prediction > 1 - self.epsilon)] = 1 - self.epsilon\n",
    "        # WRITE CODE HERE\n",
    "        pass\n",
    "        return 0\n",
    "\n",
    "    def compute_loss_and_accuracy(self, X, y):\n",
    "        one_y = self.one_hot(y)\n",
    "        cache = self.forward(X)\n",
    "        predictions = np.argmax(cache[f\"Z{self.n_hidden + 1}\"], axis=1)\n",
    "        accuracy = np.mean(y == predictions)\n",
    "        loss = self.loss(cache[f\"Z{self.n_hidden + 1}\"], one_y)\n",
    "        return loss, accuracy, predictions\n",
    "\n",
    "    def train_loop(self, n_epochs):\n",
    "        X_train, y_train = self.train\n",
    "        y_onehot = self.one_hot(y_train)\n",
    "        dims = [X_train.shape[1], y_onehot.shape[1]]\n",
    "        self.initialize_weights(dims)\n",
    "\n",
    "        n_batches = int(np.ceil(X_train.shape[0] / self.batch_size))\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            for batch in range(n_batches):\n",
    "                minibatchX = X_train[self.batch_size * batch:self.batch_size * (batch + 1), :]\n",
    "                minibatchY = y_onehot[self.batch_size * batch:self.batch_size * (batch + 1), :]\n",
    "                # WRITE CODE HERE\n",
    "                pass\n",
    "\n",
    "            X_train, y_train = self.train\n",
    "            train_loss, train_accuracy, _ = self.compute_loss_and_accuracy(X_train, y_train)\n",
    "            X_valid, y_valid = self.valid\n",
    "            valid_loss, valid_accuracy, _ = self.compute_loss_and_accuracy(X_valid, y_valid)\n",
    "\n",
    "            self.train_logs['train_accuracy'].append(train_accuracy)\n",
    "            self.train_logs['validation_accuracy'].append(valid_accuracy)\n",
    "            self.train_logs['train_loss'].append(train_loss)\n",
    "            self.train_logs['validation_loss'].append(valid_loss)\n",
    "\n",
    "        return self.train_logs\n",
    "\n",
    "    def evaluate(self):\n",
    "        X_test, y_test = self.test\n",
    "        # WRITE CODE HERE\n",
    "        pass\n",
    "        return 0\n",
    "    \n",
    "d = NN(hidden_dims=(101, 102, 103))\n",
    "d.train_loop(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "2wEqhXuIdAyG",
    "outputId": "e18d23f3-d148-4968-cbfb-8c7acc72028d",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UhR33ZXEZfG",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "y = [-23.71575487, -12.11820735, -20.87167206, -15.23032892,  -0.99521008,\n",
    " -21.896061,   11.34206525,   1.0465467,  -13.78378035, -27.32947509,\n",
    "  20.63995156,  22.25066691, -24.31994894,   0.40690604, -18.51855023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "xVfpFqU3PAJ9",
    "outputId": "47d1fab7-5c68-4046-f769-7681406884a6",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "np.exp(y - np.max(y))/np.exp(y - np.max(y)).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8mBxdGNKPEIq",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 6]\n",
      "[[3 6]]\n",
      "[[-2 -1  0]\n",
      " [-2 -1  0]]\n",
      "[[-2 -1  0]\n",
      " [-2 -1  0]]\n"
     ]
    }
   ],
   "source": [
    "x =  np.array([[1,2,3],\n",
    "               [4,5,6]])\n",
    "\n",
    "print(np.amax(x, axis=1))\n",
    "print(np.amax(x, axis=1).reshape((1,2)))\n",
    "print(x - np.amax(x, axis=1).reshape((1,2)).T)\n",
    "\n",
    "max_ = np.amax(x, axis=1).reshape(1, x.shape[0]).T\n",
    "print(x - max_)\n",
    "#np.exp(x - np.amax(x, axis=1))/np.sum(np.exp(x - np.amax(x, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in reversed(range(1, 5)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
